{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e86614-a869-49d6-9a77-cbef0b976ac0",
   "metadata": {},
   "source": [
    "embedding is nothing but a nemeric representation of our text, and we store these embeddings to our vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff82c0-4bef-4d94-b4cb-e84a9285ecfc",
   "metadata": {},
   "source": [
    "first create UPI key from Google palm makersuite and copy it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f2e79-159e-4b20-a84d-d667d71254d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3990cf-a8b9-4131-bac5-0a5c4dca18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "api_keys = \"----\"\n",
    "llm = GooglePalm(google_api_key = api_key, temperature = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d3266c-81ed-410a-8c78-da7673d5e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can ask for any input as its a generative model\n",
    "poem = llm(\"write a 4 line poem of my love for samosa\")\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7139de-8b47-4931-a452-256f90372c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = llm(\"write email requesting refund for electronic item\")\n",
    "print(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b7f7d-b31b-4406-b7d6-1fb46c7c8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can import csv file into Jupyter\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader(file_path = \"file_name.csv\", source_column = \"column_name\") #source_column = whichever columns contains questions provide that column\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a7b02-a829-494e-bda0-157a93b590e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#once csv data loaded lets create embedding\n",
    "#we use instructor embedding\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS #for creating vector database\n",
    "#from langchain.vectorstores import chroma\n",
    "\n",
    "instructor_embeddings = HuggingfaceInstructEmbeddings()\n",
    "\n",
    "vectordb= FAISS.from_documets(documents =data, embedding = instructor_embedding) #we need to specify documents here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a130c20-4d9b-4026-9bd0-e2bc82eda782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it two sentences meaning are similar then cosine similarity close to 1\n",
    "#if not similar then near to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d179a-976d-499f-9035-abae31bfac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#once vector database created we can save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1fc86-6555-4429-928e-ce434d663157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have to create a retriver object from vector database\n",
    "retriever = vectordb.as_retriever() \n",
    "rdocs = retriever.get_relevant_documents(\"for how long is this course valid\")\n",
    "rdocs\n",
    "\n",
    "#the work of this object is whenever we have new questions it wil create embedding of that question then we will pull similar looking vector from the vector database.\n",
    "#soo, its takes input question it compares its embeddings with the embeddings which are stored in vector database and then it return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b2271-7727-47e7-829c-1257b4ce0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to solve issue we have to create prompt template\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
    "In the answer try to provide as much text as possible from \"response\" section in the source document context without making.\n",
    "If the answer is not found in the context, kindly state \"I dont't know.\" Don't try to make up an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, input_variables = [\"context\", \"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9788098-5709-4dfe-99fa-33ecb3eac5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are imported retrievalQA class\n",
    "#this is performing last step which is we have relevant document now we will form this prompt and ask it to LLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "#lets create object of this class\n",
    "chain = RetrievalQA(llm = llm,\n",
    "            chain_type = \"stuff\", #is can be stuff or mapreduce\n",
    "            retriever = retriever,\n",
    "            input_key = \"query\",\n",
    "            return_source_documents = True,  #when we get answer then we also want to return the source document from that csv file which were relevant to this answer\n",
    "            chain_type_kwargs={\"prompt\":PROMPT}\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc83f1b-9b10-43ea-8659-9ce8650980e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(\"ask question\")\n",
    "#here hellucination concept happens\n",
    "#we want to tell model that only use csv for giving the answers and dont use any other source\n",
    "#so its keep saying random things, and we need to stop this\n",
    "#soo we just want to make it simple, follow csv if answer not present simply answer I Dont Know\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b106085-34b8-400b-8015-25a3f8fba826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
